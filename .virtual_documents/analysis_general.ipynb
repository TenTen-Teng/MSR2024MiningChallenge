


# Import libraries.

import pandas as pd
from langdetect import detect
import numpy as np
import altair as alt
import spacy
from multiprocessing.dummy import Pool
from tqdm import tqdm





df = pd.read_csv("~/data/DevGPT/cleaned/combine.csv")
df.info()


df.head()





# Group by RepoName
df_repo = df.groupby(by=["RepoName"])
df_repo.head()


# In our merged dataset, there are 66 unique repos.
len(df_repo["RepoName"].unique())


# Show the distribution of interactions

df_repo_count = df_repo["URL_chatgptsharing"].aggregate(len).reset_index()

idx_max = np.argmax(df_repo_count["URL_chatgptsharing"])
idx_min = np.argmin(df_repo_count["URL_chatgptsharing"])
print(
    f"Average interactions per pr: {np.mean(df_repo_count["URL_chatgptsharing"]):.2f}\n"
    f"Maximum number of interactions: {df_repo_count.iloc[idx_max]["URL_chatgptsharing"]}\n"
    f"Minimum number of interactions: {df_repo_count.iloc[idx_min]["URL_chatgptsharing"]}"
)


# Checking histogarm

inter_bar = alt.Chart(df_repo_count).mark_bar(size=5).encode(
    x=alt.X(
        "URL_chatgptsharing:N",
        title="Number of Interactions with ChatGPT",
    ),
    y=alt.Y("count()", title="Count"),
).properties(
    title={
        "text": "Distribution of the number of interactions with ChatGPT",
        "subtitle": "Merged dataset: issues, discussions, and pull requests"
    }
)

rule = alt.Chart(df_repo_count).mark_rule().encode(
    x=alt.X(
        'URL_chatgptsharing:N',
        aggregate="mean",
        type='nominal',
        axis=alt.Axis(format="2d")
    ),
    size=alt.value(2),
)

alt.layer(inter_bar, rule)





# Load NLP model.

nlp = spacy.load("en_core_web_sm")


# Get all prompts.

prompts = df["Prompt"].drop_duplicates().to_list()


# Remove punctuation
with Pool() as pool:
    doc = pool.map(nlp, tqdm(prompts, total=len(prompts)))
    


tokens = []
for d in tqdm(doc):
    token_without_punc = [token for token in d if not token.is_punct]
    tokens.append(token_without_punc)


# Calculate average length of prompt
ls_len_promot = [len(token) for token in tokens]
idx_max = np.argmax(ls_len_promot)
idx_min = np.argmin(ls_len_promot)
print(
    f"Average length of prompt per pr: {np.mean(ls_len_promot):.2f}\n"
    f"---------------------------\n"
    f"Maximum length of prompt: {prompts[idx_max]}\n"
    f"---------------------------\n"
    f"Minimum length of prompt: {prompts[idx_min]}"
)


# Create a dataframe for histogram

df_prompt_count = pd.DataFrame(ls_len_promot, columns=["length_prompt"])
df_prompt_count.head()


# Checking histogarm
inter_bar = alt.Chart(df_prompt_count).mark_bar(size=5).encode(
    x=alt.X(
        "length_prompt:N",
        title="Length of Prompts (tokens)",
    ),
    y=alt.Y("count()", title="Count"),
).properties(
    title={
        "text": "Distribution of the length of prompt with ChatGPT",
        "subtitle": "Merged dataset: issues, discussions, and pull requests"
    }
)

rule = alt.Chart(df_prompt_count).mark_rule().encode(
    x=alt.X(
        'length_prompt:N',
        aggregate="mean",
        type='nominal',
        axis=alt.Axis(format="2d")
    ),
)

alt.layer(inter_bar, rule)





# Get all answer.

answers = df["Answer"].drop_duplicates().to_list()


# Remove punctuation
with Pool() as pool:
    doc = pool.map(nlp, tqdm(answers, total=len(answers)))


tokens = []
for d in tqdm(doc):
    token_without_punc = [token for token in d if not token.is_punct]
    tokens.append(token_without_punc)


# Calculate average length of prompt
ls_len_answer = [len(token) for token in tokens]
idx_max = np.argmax(ls_len_answer)
idx_min = np.argmin(ls_len_answer)
print(
    f"Average length of answer per pr: {np.mean(ls_len_answer):.2f}\n"
    f"---------------------------\n"
    f"Maximum length of answer: {answers[idx_max]}\n"
    f"---------------------------\n"
    f"Minimum length of answer: {answers[idx_min]}"
)


# Create a dataframe for histogram

df_answer_count = pd.DataFrame(ls_len_promot, columns=["length_answer"])
df_answer_count.head()


# Checking histogarm
inter_bar = alt.Chart(df_answer_count).mark_bar(size=5).encode(
    x=alt.X(
        "length_answer:N",
        title="Length of Answer (tokens)",
    ),
    y=alt.Y("count()", title="Count"),
).properties(
    title={
        "text": "Distribution of the length of answer with ChatGPT",
        "subtitle": "Merged dataset: issues, discussions, and pull requests"
    }
)

rule = alt.Chart(df_answer_count).mark_rule().encode(
    x=alt.X(
        'length_answer:N',
        aggregate="mean",
        type='nominal',
        axis=alt.Axis(format="2d")
    ),
)

alt.layer(inter_bar, rule)
