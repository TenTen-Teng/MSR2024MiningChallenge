# MSR2024MiningChallenge

Data 542 Project - group 10
- Teng Li(34291997) 
- Haoxiang(Lance) Xu(38017547) 
- Amali Jayatileke(28697027) 

## Table of Contents
- [Introduction](#introduction)
- [Repository Structure](#repository-structure)
- [How to Run](#how-to-run)

## Introduction
 
The DevGPT dataset consists of six JSON files covering GitHub Issues, Pull Requests, Discussions, Commits, Code Files, and Hacker News threads. 

In this project, our group focuses on uncovering insights from Pull Requests, Issues, and Discussions. Rather than analyzing the entire dataset, which varies in programming languages, communication languages, and dates, we narrow our scope to data collected on July 27, 2023, specifically for Python and English. 

Below are the four research questions we aim to explore using data wrangling and data analytics techniques. 

## Repository Structure

``` bash
project_name/
│
├── __pycache__/
├── .ipynb_checkpoints/
│
├── .virtual_documents/
│   ├── analysis_general.ipynb
│   ├── data_preprocessing.ipynb
│
├── graphs/                         # Directory for generated graphs
|
│── analysis_general.ipynb
│── Data_542_Project_Data_Analysis_Issues.ipynb
│── Data_542_Project_Word_Cloud_LDA.ipynb
│── data_preprocessing.ipynb
│── discussion.ipynb
│
├── helper.py                       # Python script for helper functions
├── pull_request_analysis.ipynb     # Jupyter notebook for pull request analysis
├── README.md                       # Project documentation file
```

- `graphs/`: Generated Altair graphs are all saved in this directory.

- `analysis_general.ipynb`: Jupyter notebook for general analysis and merging of data.

- Data Analysis Notebooks:
    - `Data_542_Project_Data_Analysis_Issues.ipynb`: Jupyter notebook for data analysis on issues.
    - `pull_request_analysis.ipynb`: Jupyter notebook for data analysis on pull requests.
    - `discussion.ipynb`: Jupyter notebook for data analysis on discussions.
    - `Data_542_Project_Word_Cloud_LDA.ipynb`: Jupyter notebook for word cloud and LDA analysis.

- `helper.py`: Python helper script for extracting data from JSON files and merging into dataframes.
- `data_preprocessing.ipynb`: extracting data from JSON by different types and merging into dataframes.

## How to Run

- **Step 1** Clone the Data repository: <span style="opacity:0.64">Clone the dataset repository from the following link:</span>[DevGPT](https://github.com/NAIST-SE/DevGPT)

    OR

```bash
git clone https://github.com/NAIST-SE/DevGPT.git
```

- **Step 2** Clone the project repository: <span style="opacity:0.64">Clone this repository by running the following command in the terminal:</span> 

``` bash
git clone https://github.com/TenTen-Teng/MSR2024MiningChallenge.git
```

- **Step 3** Preprocess the data: <span style="opacity:0.64">In order to run the Jupyter notebooks, there should be several preprocessed csv files ready. To generate these files, run the `data_preprocessing.ipynb` notebook. </span>
    - **Step 3.1** <span style="opacity:0.64">Make sure to target the correct path to the DevGPT dataset in the `data_preprocessing.ipynb` notebook.</span>
    - **Step 3.2** <span style="opacity:0.64">Before running all code cells, make sure to choose the desired data type to do that change variable `target` to one of "hn", "pr", "issue", "discussion", "commit", "file".</span>
    - **Step 3.3** <span style="opacity:0.64">After running all code cells, the preprocessed csv files will be saved in the `cleaned/` directory under the DevGPT Repository folder.</span>

- **Step 4** Data Analysis: <span style="opacity:0.64">Now, all Jupyter notebooks for specific topics are available for you to run. Before running the notebooks, the file name should be formated as `topicName_total.csv`, make sure to change the path to the preprocessed CSV files in the notebooks. For different research questions, run the corresponding notebook, and ensure that their preprocessed data is also generated by their target type.</span>

- **Step 5** Final Analysis: <span style="opacity:0.64">The `analysis_general.ipynb` notebook is available for general analysis and merging of data. This notebook can be run after all the csv files are generated.</span>
    - **Step 5.1** <span style="opacity:0.64">Go back to `data_preprocessing.ipynb` in the last section; there is a code block used to combine all the CSV files into a merged dataframe for final analysis. Before running this, ensure you have `pr_total.csv`, `issue_total.csv`, and `discussion_total.csv`.`</span>
    - **Step 5.2** <span style="opacity:0.64">After running all code cells, the final analysis will be saved in the `cleaned/` directory under the DevGPT Repository folder, named `combined.csv`.</span>

